#!/bin/sh
 
#SBATCH --job-name=9classes_cosine_1k
#SBATCH --output=9classes_cosine_1k-%A.out  # Standard output of the script (Can be absolute or relative path). %A adds the job id to the file name so you can launch the same script multiple times and get different logging files
#SBATCH --error=9classes_cosine_1k-%A.err  # Standard error of the script
#SBATCH --time=0-15:00:00  # Limit on the total run time (format: days-hours:minutes:seconds)
#SBATCH --gres=gpu:1  # Number of GPUs if needed
#SBATCH --cpus-per-task=1  # Number of CPUs (Don't use more than 12/6 per GPU)
#SBATCH --mem=48G  # Memory in GB (Don't use more than 48/24 per GPU unless you absolutely need it and know what you are doing)
 
# run the program
ml cuda  # load default CUDA module

python image_train.py --data_dir /home/data/farid/vessel_segmentation/kaggle_dataset/ --dataset_mode nifti --lr 1e-4 --batch_size 1 --attention_resolutions 16,8 --diffusion_steps 1000 --image_size 64 --learn_sigma True --noise_schedule cosine --num_channels 128 --num_head_channels 64 --num_res_blocks 2 --resblock_updown True --use_fp16 True --use_scale_shift_norm True --use_checkpoint True --num_classes 9 --class_cond True --no_instance True --lr_anneal_steps 10000

ml -cuda  # unload all modules